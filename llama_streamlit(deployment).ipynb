{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing libraries"
      ],
      "metadata": {
        "id": "4LmxoRzCK8yT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2TcmNHyj_F3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae26eb3-7af0-4c6c-f5dd-7d64a32cda44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install streamlit -q # fro streamlit deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGKGdu8ymz_u",
        "outputId": "5b527f71-abc0-4b26-8d4d-2128e517fa52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -Uqq  git+https://github.com/huggingface/peft.git # for the model trained model deployment.\n",
        "!pip install -Uqq transformers datasets accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_PDWKGbm-Q8",
        "outputId": "5fa12a82-aaf1-4e61-cbe9-703fee4584d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.29.2\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2023.7.22)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.33.1\n",
            "    Uninstalling transformers-4.33.1:\n",
            "      Successfully uninstalled transformers-4.33.1\n",
            "Successfully installed transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.29.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamlit App"
      ],
      "metadata": {
        "id": "ddOlzes2LmLU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_My_SCvkUpe",
        "outputId": "1fc886de-a196-4f94-87a2-f96d9c140624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "#for the cpu code.\n",
        "#device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#generated = generated.to(device)\n",
        "#model.to(device)\n",
        "#model.eval();\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"HEALTHQUERY\")\n",
        "\n",
        "\n",
        "model = '3B' # Pick your poison'\n",
        "\n",
        "\n",
        "if model == '7B':\n",
        "    model_name = (\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\",\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\")\n",
        "    run_name = 'redpj7B-lora-int8-alpaca'\n",
        "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
        "    peft_name = 'redpj7B-lora-int8-alpaca'\n",
        "    output_dir = 'redpj7B-lora-int8-alpaca-results'\n",
        "else: #3B\n",
        "    model_name = (\"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\"togethercomputer/RedPajama-INCITE-Base-3B-v1\")\n",
        "    run_name = 'redpj3B-lora-int8-alpaca'\n",
        "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
        "    peft_name = 'redpj3B-lora-int8-alpaca'\n",
        "    output_dir = 'redpj3B-lora-int8-alpaca-results'\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# load base LLM model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name[0],\n",
        "    load_in_8bit=True,\n",
        "\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name[1])\n",
        "tokenizer.pad_token_id = 0\n",
        "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "  return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "\n",
        "input_text = st.text_input(\"Please Provide your text:\")\n",
        "\n",
        "if len(input_text)>0:\n",
        "  def generate(instruction,input=None,maxTokens=150):\n",
        "      prompt = generate_prompt({'instruction':instruction,'input':input})\n",
        "      input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "      outputs = model.generate(input_ids=input_ids, max_new_tokens=maxTokens,\n",
        "                              do_sample=True, top_p=0.9,pad_token_id=tokenizer.eos_token_id,\n",
        "                              forced_eos_token_id=tokenizer.eos_token_id,remove_invalid_values=True)\n",
        "      outputs = outputs[0].tolist()\n",
        "      # Stop decoding when hitting the EOS token\n",
        "      if tokenizer.eos_token_id in outputs:\n",
        "          eos_index = outputs.index(tokenizer.eos_token_id)\n",
        "          decoded = tokenizer.decode(outputs[:eos_index])\n",
        "          # Don't show the prompt template\n",
        "          sentinel = \"### Response:\"\n",
        "          sentinelLoc = decoded.find(sentinel)\n",
        "          if sentinelLoc >= 0:\n",
        "              st.write(decoded[sentinelLoc+len(sentinel):])\n",
        "          else:\n",
        "              print('Warning: Expected prompt template to be emitted.  Ignoring output.')\n",
        "      else:\n",
        "          print('Warning: no <eos> detected ignoring output')\n",
        "\n",
        "  # peft_model_id = f'johnrobinsn/{peft_name}' # By default use my pretrained adapter weights\n",
        "  peft_model_id = '/content/drive/MyDrive/All models/llama_redit'# Uncomment to use locally saved adapter weights if you trained above\n",
        "\n",
        "  # Load the LoRA model\n",
        "  model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
        "  model.eval()\n",
        "  torch.manual_seed(42)\n",
        "  title = input_text\n",
        "  st.write(generate(title,maxTokens=150))\n",
        "\n",
        "else:\n",
        "  st.write('Welcome to HEALTHQUERY')\n",
        "\n",
        "\n",
        "# Display output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4JHH-GWl1b6",
        "outputId": "f881b86e-4001-404e-ef0f-def25bda6b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==5.2.1\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==5.2.1) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19772 sha256=6d24838e4202f82d736715eb7e34194fbac1e84ec9f9223009f4922425ae1ddc\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e1/46/8d60711cb43fb2e055fb69bb9964f91c9a5046f7924d2996ac\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok==5.2.1\n",
        "\n",
        "\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blyEQDaimMrF",
        "outputId": "afde0aaf-6e4e-4703-e661-a57aa1dcdb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "ngrok.set_auth_token(\"2LoeoedCqusM7VxjDHJfm1b9TJg_5wRMcFESTSHY6u26tUoMj\") #ngrok.com\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACPcBTNDmRMm"
      },
      "outputs": [],
      "source": [
        "!killall ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxxFRs7GmRoz",
        "outputId": "afa51fb3-7381-481b-eb9c-701391084ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NgrokTunnel: \"http://2b0c-34-168-44-182.ngrok-free.app\" -> \"http://localhost:80\"\n"
          ]
        }
      ],
      "source": [
        "!nohup streamlit run app.py --server.port 80 &\n",
        "url = ngrok.connect(port = '80')\n",
        "print(url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}